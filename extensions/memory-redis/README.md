# Memory (Redis) Plugin

Redis-backed long-term memory plugin for OpenClaw with vector similarity search.

## Requirements

- **Redis 8+** (includes Query Engine with vector search built-in)
- **Docker** (required for curl/npm installs - native packages only have Redis 7.x)
- Embedding provider (OpenAI, Google Gemini, Ollama, or local node-llama-cpp)

## Quick Start

### 1. Start Redis

Choose the option that matches your deployment:

#### Docker (required for curl/npm installs)

Redis 8 with built-in vector search is only available via Docker. Native packages (apt/yum) are still on Redis 7.x.

```bash
# Redis 8 with persistence and vector search
docker run -d \
  --name openclaw-redis \
  -p 127.0.0.1:6379:6379 \
  -v openclaw-redis-data:/data \
  --restart unless-stopped \
  redis:8 \
  --appendonly yes --maxmemory 512mb --maxmemory-policy noeviction
```

For Docker Compose deployments, see [Docker Compose Setup](#docker-compose-setup) below.

#### Native Installation (not recommended)

> **Warning:** Native Redis Stack packages are stuck at version 7.x and may not be available for newer distros (e.g., Ubuntu 24.04). Use Docker instead.

If you must use native packages:

```bash
# Add Redis repository
curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/redis.list

# Install Redis Stack (includes Query Engine for vector search)
sudo apt-get update
sudo apt-get install -y redis-stack-server

# Start and enable
sudo systemctl enable redis-stack-server
sudo systemctl start redis-stack-server
```

#### Docker Compose Setup

**Option A: With OpenClaw Docker deployment**

```bash
# From repo root - merges Redis with OpenClaw gateway
docker compose -f docker-compose.yml -f extensions/memory-redis/docker/docker-compose.yml up -d
```

This starts Redis 8 alongside the gateway on a shared network. Use `redis://openclaw-redis:6379` as the Redis URL in your plugin config.

**Option B: Standalone compose (Redis only)**

```bash
cd extensions/memory-redis/docker && docker compose up -d
```

Access via `redis://localhost:6379` from your host.

### 2. Configure

Activate the plugin by setting `plugins.slots.memory = "memory-redis"` in your config.

**Option A: Local Embeddings (no API key needed)**

```json5
{
  plugins: {
    slots: { memory: "memory-redis" },
    entries: {
      "memory-redis": {
        config: {
          redis: { url: "redis://localhost:6379" },
          embedding: { provider: "local" },
          autoRecall: true,
          autoCapture: true,
        },
      },
    },
  },
}
```

**Option B: OpenAI Embeddings**

```json5
{
  plugins: {
    slots: { memory: "memory-redis" },
    entries: {
      "memory-redis": {
        config: {
          redis: { url: "${REDIS_URL}" },
          embedding: {
            provider: "openai",
            apiKey: "${OPENAI_API_KEY}",
            model: "text-embedding-3-small",
          },
          autoRecall: true,
          autoCapture: true,
        },
      },
    },
  },
}
```

**Option C: Auto (tries local first, falls back to OpenAI)**

```json5
{
  plugins: {
    slots: { memory: "memory-redis" },
    entries: {
      "memory-redis": {
        config: {
          redis: { url: "redis://localhost:6379" },
          embedding: {
            provider: "auto",
            fallback: "openai",
            apiKey: "${OPENAI_API_KEY}", // Only used if fallback triggers
          },
          autoRecall: true,
        },
      },
    },
  },
}
```

## Configuration Options

| Option               | Type    | Default                 | Description                                |
| -------------------- | ------- | ----------------------- | ------------------------------------------ |
| `redis.url`          | string  | required                | Redis connection URL                       |
| `redis.password`     | string  | -                       | Redis password (optional)                  |
| `redis.tls`          | boolean | false                   | Enable TLS connection                      |
| `embedding.provider` | string  | `auto`                  | `openai`, `gemini`, `local`, or `auto`     |
| `embedding.apiKey`   | string  | -                       | API key (required for openai/gemini)       |
| `embedding.model`    | string  | -                       | Embedding model (provider-specific)        |
| `embedding.baseUrl`  | string  | -                       | Custom API base URL (for Ollama/self-host) |
| `embedding.fallback` | string  | `none`                  | Fallback provider if primary fails         |
| `indexName`          | string  | `idx:openclaw:memories` | RediSearch index name                      |
| `keyPrefix`          | string  | `openclaw:memory`       | Redis key prefix                           |
| `autoCapture`        | boolean | false                   | Auto-capture important info                |
| `autoRecall`         | boolean | false                   | Auto-inject relevant memories              |

### Embedding Providers

| Provider | Default Model                    | Any Model?                          | API Key |
| -------- | -------------------------------- | ----------------------------------- | ------- |
| `openai` | `text-embedding-3-small`         | ✅ Yes, any OpenAI-compatible model | Yes     |
| `gemini` | `gemini-embedding-001`           | ✅ Yes, any Gemini embedding model  | Yes     |
| `local`  | `embeddinggemma-300M-Q8_0.gguf`  | ✅ Yes, any GGUF embedding model    | No      |
| `auto`   | Tries local first, then fallback | -                                   | Depends |

### Self-Hosted Embeddings with Ollama

You can use Ollama as a self-hosted embedding provider. Ollama exposes an OpenAI-compatible API.

**Setup Ollama on your server:**

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Configure to listen on all interfaces (edit systemd service)
sudo systemctl edit ollama
# Add: Environment="OLLAMA_HOST=0.0.0.0"

# Restart and pull embedding model
sudo systemctl restart ollama
ollama pull nomic-embed-text
```

**Configure the plugin to use Ollama:**

```json5
{
  plugins: {
    slots: { memory: "memory-redis" },
    entries: {
      "memory-redis": {
        config: {
          redis: { url: "redis://localhost:6379" },
          embedding: {
            provider: "openai",
            apiKey: "ollama", // Ollama ignores this but it's required
            model: "nomic-embed-text",
            baseUrl: "http://192.168.178.10:11434/v1",
          },
          autoRecall: true,
        },
      },
    },
  },
}
```

**Supported Ollama embedding models:**

| Model               | Dimensions | Size   | Notes                            |
| ------------------- | ---------- | ------ | -------------------------------- |
| `nomic-embed-text`  | 768        | ~275MB | Best balance of quality and size |
| `mxbai-embed-large` | 1024       | ~670MB | Higher quality, larger           |
| `all-minilm`        | 384        | ~46MB  | Fast, smaller, lower quality     |

## Agent Tools

The plugin registers these tools for the AI agent:

- **memory_recall** - Search memories with multiple modes:
  - `vector` (default): Semantic similarity search using embeddings
  - `text`: Full-text keyword search
  - `hybrid`: Combines vector and text search for best recall
- **memory_store** - Store new memories with category and importance
- **memory_forget** - Delete memories (GDPR-compliant)

## CLI Commands

```bash
# List memory count
openclaw redis-memory list

# Search memories (vector search - default)
openclaw redis-memory search "user preferences"

# Search with keyword matching
openclaw redis-memory search "dark mode" --mode text

# Hybrid search (vector + text combined)
openclaw redis-memory search "preferences" --mode hybrid --limit 10

# Show statistics
openclaw redis-memory stats

# Delete a memory
openclaw redis-memory delete <uuid>
```

## How It Works

### Storage

Memories are stored as JSON documents in Redis with vector embeddings. **Raw text is always preserved** alongside the vector embedding, enabling:

- Full-text search (in addition to vector similarity search)
- Re-embedding with new/better models in the future
- Debugging and inspecting stored memories

Each memory also tracks **who said it** (user or assistant) and **which embedding model** was used:

```json
{
  "id": "uuid",
  "text": "User prefers dark mode",
  "vector": [0.1, 0.2, ...],
  "importance": 0.7,
  "category": "preference",
  "role": "user",
  "embeddingModel": "openai/text-embedding-3-small",
  "createdAt": 1706889600000
}
```

This design ensures you can:

- Filter memories by role (e.g., "what did the user tell me?" vs "what did I conclude?")
- Migrate to a new embedding model by re-embedding all stored text
- Use hybrid search (vector + full-text) for better recall

### Vector Search

Uses RediSearch's HNSW (Hierarchical Navigable Small World) algorithm for fast approximate nearest neighbor search with cosine similarity.

### Auto-Recall

When enabled, searches for relevant memories before each agent interaction and injects them into the context.

### Auto-Capture

When enabled, analyzes conversations for important information (preferences, facts, decisions, contact info) and automatically stores them.

## Development

### Run Tests

```bash
# Unit tests (no Redis needed)
pnpm vitest run --config vitest.extensions.config.ts extensions/memory-redis

# Live tests (requires Redis)
# With local embeddings (no API key needed):
LIVE=1 REDIS_URL=redis://localhost:6379 pnpm vitest run --config vitest.extensions.config.ts extensions/memory-redis

# With OpenAI embeddings:
LIVE=1 REDIS_URL=redis://localhost:6379 OPENAI_API_KEY=sk-... pnpm vitest run --config vitest.extensions.config.ts extensions/memory-redis
```

## License

MIT
